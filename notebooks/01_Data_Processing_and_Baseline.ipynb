{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5c5e6a",
   "metadata": {},
   "source": [
    "1. Inicialización e importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c26898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las features se cargarán desde CSV.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# FIX para el ModuleNotFoundError\n",
    "if 'notebooks' in os.getcwd():\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "else:\n",
    "    sys.path.append(os.path.abspath(os.getcwd()))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Creamos las carpetas de resultados, ya que las figuras se guardarán aquí\n",
    "os.makedirs('results/figures', exist_ok=True)\n",
    "\n",
    "print(\"Las features se cargarán desde CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9f6b8",
   "metadata": {},
   "source": [
    "2. Cargar y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f441090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features cargadas exitosamente. Total de artículos: 0\n",
      "\n",
      "Muestra de Features Listas para el Modelo\n",
      "Empty DataFrame\n",
      "Columns: [X_baseline, X_ocr_model]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_FILE = 'data/processed/fakenewsnet_processed.csv'\n",
    "\n",
    "try:\n",
    "    # Cargamos el archivo que generamos en la terminal con Scraping y OCR\n",
    "    df = pd.read_csv(PROCESSED_FILE)\n",
    "    print(f\"Features cargadas exitosamente. Total de artículos: {len(df)}\")\n",
    "    \n",
    "    # 1. Definir nuestras variables X e y\n",
    "    X = df[['X_baseline', 'X_ocr_model']]\n",
    "    y = df['label']\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: No se encontró el archivo procesado en '{PROCESSED_FILE}'.\")\n",
    "    print(\"Por favor, asegúrese de haber ejecutado 'python src/data_loader.py' en la terminal.\")\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "if X is not None:\n",
    "    print(\"\\nMuestra de Features Listas para el Modelo\")\n",
    "    print(X.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041912fd",
   "metadata": {},
   "source": [
    "3. Dividir los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee22f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dividir los datos (80% entrenamiento, 20% prueba)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pasamos ambas columnas\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatos de entrenamiento: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDatos de prueba: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Dividir los datos (80% entrenamiento, 20% prueba)\n",
    "if X is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, # Pasamos ambas columnas\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y \n",
    "    )\n",
    "\n",
    "    print(f\"Datos de entrenamiento: {len(X_train)}\")\n",
    "    print(f\"Datos de prueba: {len(X_test)}\")\n",
    "else:\n",
    "    print(\"No se puede ejecutar la división de datos (X es nulo).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19167c7c",
   "metadata": {},
   "source": [
    "4. Experimento A: Modelo baseline solo texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "\n",
    "    # 1. Vectorizador\n",
    "    vectorizer_base = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "    # 2. Vectorizar la feature X_baseline \n",
    "    X_train_vec_base = vectorizer_base.fit_transform(X_train['X_baseline'])\n",
    "    X_test_vec_base = vectorizer_base.transform(X_test['X_baseline'])\n",
    "\n",
    "    # 3. Modelo y Entrenamiento\n",
    "    model_base = RandomForestClassifier(random_state=42)\n",
    "    model_base.fit(X_train_vec_base, y_train)\n",
    "\n",
    "    # 4. Evaluación\n",
    "    print(\"\\nRESULTADOS:\")\n",
    "    y_pred_base = model_base.predict(X_test_vec_base)\n",
    "    print(classification_report(y_test, y_pred_base, target_names=['REAL (0)', 'FAKE (1)']))\n",
    "\n",
    "    # 5. Matriz de Confusión\n",
    "    ConfusionMatrixDisplay.from_estimator(model_base, X_test_vec_base, y_test, display_labels=['REAL', 'FAKE'])\n",
    "    plt.title('Matriz de Confusión - Baseline')\n",
    "    plt.savefig('results/figures/confusion_matrix_baseline.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f99cb0",
   "metadata": {},
   "source": [
    "5. Experimento B: Texto + OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "\n",
    "    # 1. Vectorizador\n",
    "    vectorizer_ocr = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "    # 2. Vectorizar la feature X_ocr_model \n",
    "    X_train_vec_ocr = vectorizer_ocr.fit_transform(X_train['X_ocr_model'])\n",
    "    X_test_vec_ocr = vectorizer_ocr.transform(X_test['X_ocr_model'])\n",
    "\n",
    "    # 3. Modelo y Entrenamiento\n",
    "    model_ocr = RandomForestClassifier(random_state=42)\n",
    "    model_ocr.fit(X_train_vec_ocr, y_train)\n",
    "\n",
    "    # 4. Evaluación\n",
    "    print(\"\\n--- RESULTADOS: MODELO PROPUESTO (Texto + OCR) ---\")\n",
    "    y_pred_ocr = model_ocr.predict(X_test_vec_ocr)\n",
    "    print(classification_report(y_test, y_pred_ocr, target_names=['REAL (0)', 'FAKE (1)']))\n",
    "\n",
    "    # 5. Matriz de Confusión\n",
    "    ConfusionMatrixDisplay.from_estimator(model_ocr, X_test_vec_ocr, y_test, display_labels=['REAL', 'FAKE'])\n",
    "    plt.title('Matriz de Confusión - Propuesto (Texto + OCR)')\n",
    "    plt.savefig('results/figures/confusion_matrix_ocr.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
